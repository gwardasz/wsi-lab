{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa.\n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt**\n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ],
      "metadata": {
        "id": "cpar5LziY_-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "XNc-O3npA-J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_discretization(data, number_of_discrete_values):\n",
        "    new_data = np.zeros_like(data, dtype='int32')\n",
        "    for column_index in range(data.shape[-1]):\n",
        "        max_column_value = np.max(data[:,column_index])\n",
        "        min_column_value = np.min(data[:,column_index])\n",
        "        disrete_range = (max_column_value-min_column_value) * 1/number_of_discrete_values\n",
        "        discrete_boundaries = np.linspace(min_column_value, max_column_value, number_of_discrete_values+1)[1:-1]\n",
        "        new_data[:, column_index] = np.digitize(data[:, column_index], discrete_boundaries, right=True).astype('int32')\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "07e_lMigH5zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_discrete = data_discretization(x_train, 4)\n",
        "x_test_discrete = data_discretization(x_test, 4)"
      ],
      "metadata": {
        "id": "HSAFMwJ-AAMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)\n",
        "\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}  # discrete for each feature index {0: {0: 0.5, 1: 0.5,...},...}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        class_counter = Counter(train_classes)\n",
        "        self.priors = {key: class_items_number / len(train_classes) for key, class_items_number in class_counter.items()}\n",
        "        train_features_discrete = self.data_discretization(train_features)\n",
        "\n",
        "        for column_index in range(train_features_discrete.shape[-1]):\n",
        "            feature_counter = Counter(train_features_discrete[:,column_index])\n",
        "            self.likelihoods[column_index] = {key: feature_items_number / len(train_features_discrete) for key, feature_items_number in feature_counter.items()}  #staticmethod\n",
        "\n",
        "    def data_discretization(self, data, number_of_discrete_values=4):\n",
        "        new_data = np.zeros_like(data, dtype='int32')\n",
        "        self.discrete_boundaries = np.zeros([data.shape[-1], number_of_discrete_values-1])\n",
        "        for column_index in range(data.shape[-1]):\n",
        "            max_column_value = np.max(data[:,column_index])\n",
        "            min_column_value = np.min(data[:,column_index])\n",
        "            disrete_range = (max_column_value-min_column_value) * 1/number_of_discrete_values\n",
        "            self.discrete_boundaries[column_index] = np.linspace(min_column_value, max_column_value, number_of_discrete_values+1)[1:-1]\n",
        "            new_data[:, column_index] = np.digitize(data[:, column_index], self.discrete_boundaries[column_index], right=True).astype('int32')\n",
        "        return new_data\n",
        "\n",
        "    '''@staticmethod\n",
        "    def data_discretization_2(data, number_of_discrete_values=4):\n",
        "        new_data = np.zeros_like(data, dtype='int32')\n",
        "        for column_index in range(data.shape[-1]):\n",
        "            max_column_value = np.max(data[:,column_index])\n",
        "            min_column_value = np.min(data[:,column_index])\n",
        "            disrete_range = (max_column_value-min_column_value) * 1/number_of_discrete_values\n",
        "            discrete_boundaries = np.linspace(min_column_value, max_column_value, number_of_discrete_values+1)[1:-1]\n",
        "            new_data[:, column_index] = np.digitize(data[:, column_index], discrete_boundaries, right=True).astype('int32')\n",
        "        return new_data\n",
        "    '''\n",
        "\n",
        "    def sample_discretization(self, sample):\n",
        "        return np.array([np.digitize(feature, self.discrete_boundaries[i], right=True).astype('int32') for i,feature in enumerate(sample)])\n",
        "\n",
        "    def predict(self, sample):\n",
        "        print(sample)\n",
        "        discrete_sample = self.sample_discretization(sample)\n",
        "        likelihoods_list = [self.likelihoods[i][feature] for i, feature in enumerate(discrete_sample)]\n",
        "        print(discrete_sample)\n",
        "        print(likelihoods_list)\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}  # for each feature index fe. {0: [mean, std],...}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        class_counter = Counter(train_classes)\n",
        "        self.priors = {key: class_item_number / len(train_classes) for key, class_item_number in class_counter.items()}\n",
        "        for column_index in range(train_features.shape[-1]):\n",
        "            mean = np.mean(train_features[:,column_index])\n",
        "            std = np.sqrt(np.mean(np.power(train_features[:,column_index],2)) - np.power(mean, 2))\n",
        "            self.likelihoods[column_index] = [mean, std]\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_dist(x, mean, std):\n",
        "        return (1/(std*np.sqrt(2*np.pi))) * np.exp((-np.power(x-mean, 2))/(2*np.power(std, 2)))\n",
        "\n",
        "    def predict(self, sample):\n",
        "        print(sample)\n",
        "        likelihoods_list = [self.normal_dist(feature, self.likelihoods[i][0], self.likelihoods[i][1]) for i,feature in enumerate(sample)]\n",
        "        print(likelihoods_list)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 157,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = NaiveBayes()\n",
        "n.build_classifier(x_train, y_train)\n",
        "for i,sample in enumerate(x_test):\n",
        "    y_prediction = n.predict(sample)\n",
        "    if y_test[i] == y_prediction:\n",
        "        print(\"Success\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwm__xYfxJzj",
        "outputId": "aeb9f19d-be7a-4f21-8594-0e51b403e9ea"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.3 2.5 4.9 1.5]\n",
            "[2 0 2 2]\n",
            "[0.2740740740740741, 0.15555555555555556, 0.4074074074074074, 0.28888888888888886]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = GaussianNaiveBayes()\n",
        "g.build_classifier(x_train, y_train)\n",
        "for i,sample in enumerate(x_test):\n",
        "    y_prediction = g.predict(sample)\n",
        "    if y_test[i] == y_prediction:\n",
        "        print(\"Success\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "77Jzjtpws5sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc50d21-b3f0-48bc-8c9b-48e879ab2259"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.3 2.5 4.9 1.5]\n",
            "[0.4166742044020813, 0.3941450497781225, 0.18151741396628798, 0.48368612309930614]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2leUamPXZWu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}